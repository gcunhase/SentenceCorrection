"""
Computes the BLEU, ROUGE
using the COCO metrics scripts

Source: https://github.com/harpribot/nlp-metrics

Modified by Gwena Cunha on June 13th 2017
"""
from bleu.bleu import Bleu
from rouge.rouge import Rouge
import glob
import argparse

def load_textfiles(references, hypothesis):
    hypo = {idx: [lines.strip()] for (idx, lines) in enumerate(hypothesis)}
    # take out newlines before creating dictionary
    raw_refs = [map(str.strip, r) for r in zip(references)]
    refs = {idx: rr for idx, rr in enumerate(raw_refs)}
    # sanity check that we have the same number of references as hypothesis
    if len(hypo) != len(refs):
        raise ValueError("There is a sentence number mismatch between the inputs")
    return refs, hypo


def score(ref, hypo):
    """
    ref, dictionary of reference sentences (id, sentence)
    hypo, dictionary of hypothesis sentences (id, sentence)
    score, dictionary of scores
    """
    scorers = [
        (Bleu(4), ["Bleu_1", "Bleu_2", "Bleu_3", "Bleu_4"]),
        (Rouge(), "ROUGE_L"),
    ]
    final_scores = {}
    for scorer, method in scorers:
        score, scores = scorer.compute_score(ref, hypo)
        if type(score) == list:
            for m, s in zip(method, score):
                final_scores[m] = s
        else:
            final_scores[method] = score
    return final_scores

if __name__ == '__main__':
    # Feed in the directory where the hypothesis summary and true summary is stored
    parser = argparse.ArgumentParser(description='Translate MTGRU network')
    parser.add_argument('--scores_file', type=str, default='test-gwena/newstest2013_small2_out_mtgru_scores.txt',
                        help='Text file with BLEU and ROUGE scores.')
    parser.add_argument('--hyp_file', type=str, default='test-gwena/newstest2013_small2_out_mtgru.txt',
                        help='Text file of sentences generated by the model.')
    parser.add_argument('--ref_file', type=str, default='test-gwena/newstest2013_small2.fr',
                        help='Text file of reference sentences.')
    args = parser.parse_args()    
    
    hyp_file = args.hyp_file    
    ref_file = args.ref_file    
    scores_file = args.scores_file    
    
    #Small test file: 16 sentences
    #hyp_file = 'test-gwena/newstest2013_small2_out_gru.txt' #hypothesis
    #hyp_file = 'test-gwena/newstest2013_small2_out_mtgru.txt' #hypothesis
    #ref_file = 'test-gwena/newstest2013_small2.fr' #reference

    #Big test file: 1,000 sentences
    #hyp_file = 'test-gwena/newstest2013_1000_out_gru.txt' #hypothesis
    #hyp_file = 'test-gwena/newstest2013_1000_out_mtgru.txt' #hypothesis
    #ref_file = 'test-gwena/newstest2013_1000.fr' #reference

    hf = open(hyp_file,"r")
    #hypothesis = hf.readlines()
    hypothesis = hf.read().split("\n")
    num_sentences = len(hypothesis)-1
    
    rf = open(ref_file,"r")
    reference = rf.read().split("\n")
    
    sf = open(scores_file,"w")
    
    BLEU_1 = 0.
    BLEU_2 = 0.
    BLEU_3 = 0.
    BLEU_4 = 0.
    ROUGE_L = 0.
    for i in range(0, num_sentences):
        #print("Hypothesis " + str(i+1) + ": " + hypothesis[i])
        #print("Reference "+ str(i+1) + ": " + reference[i])

        ref, hypo = load_textfiles(reference, hypothesis)
        score_map = score(ref, hypo)
        BLEU_1 += score_map['Bleu_1']
        BLEU_2 += score_map['Bleu_2']
        BLEU_3 += score_map['Bleu_3']
        BLEU_4 += score_map['Bleu_4']
        ROUGE_L += score_map['ROUGE_L']

    scores_str = 'Bleu - 1gram: '+str(BLEU_1/num_sentences)+'\nBleu - 2gram: '+str(BLEU_2/num_sentences)+'\nBleu - 3gram: '+str(BLEU_3/num_sentences)+'\nBleu - 4gram: '+str(BLEU_4/num_sentences)+'\nRouge: '+str(ROUGE_L/num_sentences)
    
    sf.write(scores_str)    
    sf.close()
    
    print 'Average Metric Score for All Review Summary Pairs:'
    print scores_str
    
    
