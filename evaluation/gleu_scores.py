#
# Source: http://www.nltk.org/api/nltk.translate.html
#
# Author: Gwena Cunha
# Date: Jan 23rd 2018
#

import nltk.translate.gleu_score as gleu
import glob
import argparse

def test():
  hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']
  ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']
  ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which', 'guarantees', 'the', 'military', 'forces', 'always', 'being', 'under', 'the', 'command', 'of', 'the', 'Party']
  ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army', 'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']

  hyp2 = str('he read the book because he was interested in world history').split()
  ref2a = str('he was interested in world history because he read the book').split()

  list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
  hypotheses = [hyp1, hyp2]
  corpus_score = gleu.corpus_gleu(list_of_references, hypotheses) 
  print("Corpus score: "+str(corpus_score))

  #The example below show that corpus_gleu() is different from averaging sentence_gleu() for hypotheses
  score1 = gleu.sentence_gleu([ref1a], hyp1)
  score2 = gleu.sentence_gleu([ref2a], hyp2)
  average_score = (score1 + score2) / 2 
  print("Sentence score average: "+str(average_score))


if __name__ == "__main__":
  test()

  # Feed in the directory where the hypothesis summary and true summary is stored
  parser = argparse.ArgumentParser(description='Translate MTGRU network')
  #parser.add_argument('--scores_file', type=str, default='gleu_scores.txt', help='Text file with GLEU scores.')
  #parser.add_argument('--hyp_file', type=str, default='hyp.txt', help='Text file of sentences generated by the model.')
  #parser.add_argument('--ref_file', type=str, default='ref.txt', help='Text file of reference sentences.')
  parser.add_argument('--scores_file', type=str, default='WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps-1-0.999-0.998/newstest2013_1000_out_mtgru_gleu_scores.txt', help='Text file with GLEU scores.')
  parser.add_argument('--hyp_file', type=str, default='WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps-1-0.999-0.998/newstest2013_1000_out_mtgru.txt', help='Text file of sentences generated by the model.')
  parser.add_argument('--ref_file', type=str, default='WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps/newstest2013_1000.fr', help='Text file of reference sentences.')
  #parser.add_argument('--scores_file', type=str, default='test-gwena/newstest2013_small2_out_mtgru_scores.txt', help='Text file with BLEU and ROUGE scores.')
  #parser.add_argument('--hyp_file', type=str, default='test-gwena/newstest2012013_small2_out_mtgru.txt', help='Text file of sentences generated by the model.')
  #parser.add_argument('--ref_file', type=str, default='test-gwena/newstest2013_small2.fr', help='Text file of reference sentences.')

#python gleu_gwena.py --scores_file='WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps/newstest2013_1000_out_rnn_gleu_scores.txt' --hyp_file='WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps/newstest2013_1000_out_rnn.txt' --ref_file= 'WCCI_paper_results/incompleteDataPOS-15_20wordsTest-25000steps/newstest2013_1000.fr'

  args = parser.parse_args()    
    
  hyp_file = args.hyp_file    
  ref_file = args.ref_file    
  scores_file = args.scores_file

  hf = open(hyp_file,"r")
  hypothesis = hf.read().split("\n")
  num_sentences = len(hypothesis)-1
  
  rf = open(ref_file,"r")
  reference = rf.read().split("\n")
    
  sf = open(scores_file,"w")

  gleu_score_average = 0.0
  
  real_num_sentences = 0
  for i in range(0, num_sentences):
    if (len(reference[i].strip()) != 0 or len(hypothesis[i].strip()) != 0):
      print("Ref" +str(i)+": "+reference[i])
      print("Hyp" +str(i)+": "+hypothesis[i])
      ref, hypo = reference[i].split(), hypothesis[i].split()
      gleu_score_average = gleu_score_average + gleu.sentence_gleu([ref], hypo)
      real_num_sentences = real_num_sentences + 1
  
  gleu_score_average = gleu_score_average / real_num_sentences
  print("Sentences: "+str(real_num_sentences)+"; GLEU score average: "+str(gleu_score_average))

  scores_str = 'GLEU: '+str(gleu_score_average)

  sf.write(scores_str)    
  sf.close()
    
  #print 'Average Metric Score for All Review Summary Pairs:'
  #print scores_str
